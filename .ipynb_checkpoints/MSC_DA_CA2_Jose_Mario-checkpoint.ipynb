{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549c929c-11d0-4f1d-ae07-b97af747b6fc",
   "metadata": {},
   "source": [
    "# Data Preparation:\n",
    "### Data Preparation it the first step to be performed with the data. This process consists of collecting, cleaning, transforming, and manipulating raw data to make it usable for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a2067-d874-4fd1-ad8f-7ba2ae30a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "import matplotlib as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a2046-4301-4ca5-9d88-1429608a0203",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dublin Bikes datasets:\n",
    "    These datasets were collected from data.gov.ie and they are under License: Creative Commons Attribution 4.0 (CC BY 4.0).\n",
    "    As a total de 12 file were downloaded, this code will read all files and concatenate into a data frame called dublin_hist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b681b-ce88-4fc1-b3e5-d38207f12990",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "files = [file for file in os.listdir(path) if file.startswith('dublinbike-historical-data-2023') ] # filter only historical data files\n",
    "\n",
    "dublin_hist = pd.DataFrame()\n",
    "\n",
    "# for loop will load the file in a dataframe \"current_month\" and concatenate it to the dataframe \"dublin_hist\".\n",
    "for file in files:\n",
    "    current_month = pd.read_csv(path+\"/\"+file)\n",
    "    dublin_hist = pd.concat([dublin_hist, current_month])\n",
    "\n",
    "dublin_hist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744ab22-755f-47f2-abbc-829cd46ea39e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extracting Station Name and Location for a new dataframe: dublin_bike_stations\n",
    "    From the dataset, the bike stations will be extracted buy removing duplicates stations from all hist also, unicessary columns will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefee10-8feb-44e4-ab20-1d4351c53002",
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin_bike_stations = dublin_hist[['STATION ID','NAME','BIKE_STANDS','STATUS','LATITUDE', 'LONGITUDE']].drop_duplicates(subset=[\"STATION ID\"])\n",
    "\n",
    "dublin_bike_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba371e3b-fb92-463e-8084-c66d5bc4fdcf",
   "metadata": {},
   "source": [
    "### For proper analysis, only historical data and open station_id will be selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffda05a-57ac-4f25-858a-ea61c11d0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only OPEN stations and rename columns.\n",
    "dublin_hist = dublin_hist[dublin_hist['STATUS'] == 'OPEN']\n",
    "\n",
    "dublin_hist = dublin_hist[['STATION ID', 'TIME', 'BIKE_STANDS', 'AVAILABLE_BIKE_STANDS', 'AVAILABLE_BIKES']]\n",
    "\n",
    "#rename column AVAILABLE_BIKE_STANDS to BIKES_IN_USE\n",
    "dublin_hist.rename(columns={'AVAILABLE_BIKE_STANDS': 'BIKES_IN_USE'}, inplace=True)\n",
    "\n",
    "\n",
    "# Setting the TIME collumn to datetime\n",
    "dublin_hist['TIME'] = pd.to_datetime(dublin_hist['TIME'])\n",
    "dublin_hist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdab11f-2485-4bc0-beca-e670a4dcf3d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculating the number of trips \n",
    "\n",
    "    to get the number of  trips, its is necessary to create a for loop, to filter the dataframe by STATION ID and compare the number of BIKES_IN_USE from current row, to the previous row (diff),  if the number of BIKES_IN_USE is  bigger the previous row, so its a new trip otherwise not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248100c-5d61-41e2-9bf6-eb3475957883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty dataframe to receive the cleaned result\n",
    "bike_usage = pd.DataFrame()\n",
    "\n",
    "for i in dublin_bike_stations['STATION ID']:\n",
    "    # filter by STATION ID \n",
    "    fitered = dublin_hist[dublin_hist['STATION ID']== i]\n",
    "\n",
    "    # Sorting by time\n",
    "    fitered = fitered.sort_values(by=['TIME'], ascending=True)\n",
    "    \n",
    "    # creating a new collumn, \"TRIPS\" and getting the diff of each row\n",
    "    fitered[\"TRIPS\"] = fitered[\"BIKES_IN_USE\"].diff()\n",
    "    \n",
    "    # Concatenat the new bike_usage dataframe with the filtered dataframe (only with trips)\n",
    "    bike_usage = pd.concat([bike_usage, fitered[fitered['TRIPS']>0]])\n",
    "\n",
    "\n",
    "\n",
    "bike_usage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2b5d3-92e3-4b5b-b521-b42542aa9fe5",
   "metadata": {},
   "source": [
    "# Washington D.C. - Capital BikeShare Datasets\n",
    "    These datasets were collected from: https://ride.capitalbikeshare.com/system-data and they are under \"Capital Bikeshare Data License Agreement\"\n",
    "    As a total de 12 file were downloaded, this code will read all files and concatenate into a data frame called capital_df.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe98e5-8f4a-4f91-8655-6b69b05cef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./\"\n",
    "files = [file for file in os.listdir(path) if file.endswith('capitalbikeshare-tripdata.csv') ] # filter only historical data files\n",
    "\n",
    "capital_df = pd.DataFrame()\n",
    "\n",
    "# for loop will load the file in a dataframe \"current_month\" and concatenate it to the dataframe \"capital_df\".\n",
    "for file in files:\n",
    "    current_month = pd.read_csv(path+\"/\"+file)\n",
    "    capital_df = pd.concat([capital_df, current_month])\n",
    "\n",
    "capital_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370c20b-57ab-4074-bbd3-70f80d0c984a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning Capital_df dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fde30-0f1b-4734-a5dd-4404011b836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out if is there any duplicated row and null value\n",
    "\n",
    "print('duplicated rides:' ,capital_df['ride_id'].duplicated().sum())\n",
    "print('Null Values: \\n',capital_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca0802-a719-4feb-a8c0-73757d2b6708",
   "metadata": {},
   "source": [
    "    Not duplicated rides, but there are some Station_id with null values. Its necessary to fix it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab61f52-7be3-4b00-bb99-c5dc09e80d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = capital_df[capital_df.isnull().any(axis=1)]\n",
    "null_rows.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8294076-7147-42f0-a5ff-acc0cf77ec47",
   "metadata": {},
   "source": [
    "    For this dataset, each row represents one trip, and threre are other columns differente from dublin Bike, as the purpose is to compare both sistems, this dataset will be engineered to became similar to dublin bikeshare dataset.\n",
    "    As, the purpose of this project is compare the quantity of riders,  the null values will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd3313-f0df-4409-819c-541f8006c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row represents one trip\n",
    "capital_df['TRIPS']=1\n",
    "capital_df = capital_df.dropna(subset=['start_station_id'])\n",
    "capital_df['start_station_id'] = capital_df['start_station_id'].astype(int)\n",
    "\n",
    "\n",
    "# Set the right  type format of column\n",
    "capital_df['started_at'] = pd.to_datetime(capital_df['started_at'])\n",
    "capital_df.info(), display(capital_df.head(2));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d09cb-94e9-4f3c-9085-a0ec577036fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting the Bike Station  location\n",
    "capital_stations = capital_df[['start_station_id','start_station_name','start_lat','start_lng']].drop_duplicates(subset=[\"start_station_id\"])\n",
    "\n",
    "\n",
    "# rename the columns name\n",
    "col_names = {'start_station_id': 'station_id', 'start_station_name': 'station_name', 'start_lat': 'lat','start_lng':'long'}\n",
    "capital_stations.rename(columns=col_names, inplace=True)\n",
    "capital_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108b0b3-931d-4faa-87ab-d774104b2f66",
   "metadata": {},
   "source": [
    "# Reviews Datasets for sentment analysis  -  Dublin Bikes and Capital BikeShare\n",
    "    For collection of Dublin Bikes and Capital Bikeshare reviews, a tripadvisor API_KEY was created and stored in an .env file.\n",
    "    Tripeadvisor API allow to request only 5 reviews, which it is not a significant population to perform Machine Learning Sentimental Analysis.\n",
    "    Due this result a Praw library was used to collect comments from Reddit.\n",
    "    # \"Praw\" is an officially supported way to interact with the Reddit API, and Reddit's Terms of Service do allow the use for personal, non-commercial purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d845d-b221-4702-a047-9a5cce4fadfe",
   "metadata": {},
   "source": [
    "### Function to Collect reviews from TripAdvisor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a786bc7-6d52-4a50-a06e-71706f1118e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "load_dotenv()\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Function collect_trip collects reviews from tripadvisor, it receives location_id as parameter.\n",
    "def collect_trip (location_id):\n",
    "    # Setting Parameters\n",
    "    api_key = getenv('API_KEY')   \n",
    "    #location_id = \"6439815\"   # Dublin Bikes id in TripAdvisor.\n",
    "\n",
    "    url = f\"https://api.tripadvisor.com/api/partner/2.0/location/{location_id}/reviews\"\n",
    "\n",
    "    headers = { \"Content-Type\": \"application/json\", \"X-TripAdvisor-API-Key\": api_key,}\n",
    "\n",
    "    # This code makes a GET request, If the status code is 200 (Sucess), then it extracts the JSON data from the response and converts it into a pandas DataFrame. \n",
    "    # If the status code is not 200, then it prints an error message\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return pd.DataFrame( data['data'])\n",
    "       \n",
    "    else:\n",
    "        return \"Error\",response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bbfb6-f54d-4b00-b1d4-3896841b6a51",
   "metadata": {},
   "source": [
    "### Collecting reviews from TripAdvisor for Dublin Bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31bad9-875d-4b7e-b0f2-a9ad18984d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin_reviews_df = collect_trip('6439815')\n",
    "\n",
    "#Select only important features:\n",
    "dublin_reviews_df = dublin_reviews_df[['title','text']]\n",
    "dublin_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b7cfa-b2ba-45ce-b4d7-054e15e0890b",
   "metadata": {},
   "source": [
    "### Function to collect comments from Reddit using Praw Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5891da1-06cb-434a-9f9b-770fa5a1e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install praw if its not installed\n",
    "# !pip install praw\n",
    "import praw\n",
    "from datetime import datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "\n",
    "# load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function collect_reddit: Collects comment from Reddit, it receives the subreddit and the search_query as Paramter, and returns a dataframe\n",
    "def collect_reddit(subreddit,search_query):\n",
    "    \n",
    "    # Initializing Reddit Api\n",
    "    reddit = praw.Reddit(\n",
    "    client_id=getenv(\"APP_ID\"), \n",
    "    client_secret=getenv(\"AP_SECRET\"), \n",
    "    user_agent=f\"pda-jmcloudpro u/{getenv('USERNAME')}\", \n",
    "    )\n",
    "    \n",
    "    # Search from subreddit_name\n",
    "    subreddit = reddit.subreddit(subreddit)\n",
    "\n",
    "    # Perform a search inside the subreddit\n",
    "    results = subreddit.search(search_query, limit=500)  \n",
    "\n",
    "    # Creating Lists to store titles and texts\n",
    "    titles = []\n",
    "    texts = []\n",
    "\n",
    "    # Collect titles and texts\n",
    "    for submission in results:\n",
    "        titles.append(submission.title)\n",
    "        texts.append(submission.selftext)\n",
    "\n",
    "    \n",
    "    # Return a DataFrame with title and comments\n",
    "    return pd.DataFrame({'title': titles, 'text': texts})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02979a4-933d-4e55-8c24-a2755a1a8680",
   "metadata": {},
   "source": [
    "### Collecting Reddit Comments for Dublin Bikes and adding to dublin_reviews_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec368150-dad6-4052-8c5e-44f8d0e259d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin = collect_reddit('Dublin','Dublin Bikes')\n",
    "\n",
    "#Concatenate TripAdvisor Reviews and Reddit Comments.\n",
    "dublin_reviews_df = pd.concat([dublin_reviews_df,dublin], ignore_index=True)\n",
    "\n",
    "dublin_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f5db5-0089-4c03-b613-2570e4ea8dd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collecting TripAdvisor reviews for Capital BikeShare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659cd33-3b62-40a1-8a0c-97811c7bb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_reviews_df = collect_trip('2478701')\n",
    "\n",
    "#Select only important features:\n",
    "capital_reviews_df = capital_reviews_df[['title','text']]\n",
    "capital_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904ac8e-6205-4a48-a332-a4017950c0f2",
   "metadata": {},
   "source": [
    "### Collecting Reddit comments for Capital BikeShare and adding to capital_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fd13b-09e8-4c91-954e-2f7762e3738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital = collect_reddit('washingtondc','Capital Bikeshare')\n",
    "\n",
    "#Concatenate TripAdvisor Reviews and Reddit Comments.\n",
    "capital_reviews_df = pd.concat([capital_reviews_df,capital], ignore_index=True)\n",
    "\n",
    "capital_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2d4af-7762-4c3d-8196-35d66f15d40e",
   "metadata": {},
   "source": [
    "# Cleaning reviews datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516f468-fb9f-4e3d-87bb-f934f6e75a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dublin_reviews_df - Merging title and text\n",
    "dublin_reviews_df['text'] = dublin_reviews_df['title'] + ' ' + dublin_reviews_df['text']\n",
    "\n",
    "dublin_reviews_df = dublin_reviews_df.drop(columns=['title'])\n",
    "\n",
    "# removing any special characters, double spaces, tabs etc.\n",
    "dublin_reviews_df['text'] = dublin_reviews_df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "dublin_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084178f5-909b-4b2b-971c-27430831eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capital_reviews_df - Merging title and text\n",
    "\n",
    "capital_reviews_df['text'] = capital_reviews_df['title'] + ' ' + capital_reviews_df['text']\n",
    "\n",
    "capital_reviews_df = capital_reviews_df.drop(columns=['title'])\n",
    "\n",
    "# removing any special characters, double spaces, tabs etc.\n",
    "capital_reviews_df['text'] = capital_reviews_df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "capital_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c612b62-0c1b-43c7-81e9-0fe49a133d43",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)  - Understanding the data and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5823613-d6cf-4b38-b525-c3cbc2a63885",
   "metadata": {},
   "source": [
    "    Showing the top 10 stations with number of trips during the year per Station Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055cbca5-f409-4212-9ef5-33357041ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "trip_by_station_dublin = bike_usage.groupby(['STATION ID'])['TRIPS'].sum().reset_index()                      \n",
    "trip_by_station_dublin_join = pd.merge(trip_by_station_dublin, dublin_bike_stations, how='inner', on='STATION ID').sort_values(by = 'TRIPS', ascending = False)\n",
    "top_stations_dublin = trip_by_station_dublin_join.head(10)\n",
    "\n",
    "fig = px.bar(top_stations_dublin.sort_values(by='TRIPS', ascending = True), \n",
    "             x='TRIPS', \n",
    "             y='NAME',\n",
    "             labels=dict(x='TRIPS', y='NAME'),\n",
    "             title='Top 10 Bike Usage by Station - Dublin',\n",
    "             hover_data={'BIKE_STANDS': True},\n",
    "             width=1000, \n",
    "             height=500, \n",
    "             orientation='h')\n",
    "\n",
    "# Add annotations for BIKE_STANDS inside the bars\n",
    "for index, row in top_stations_dublin.iterrows():\n",
    "    fig.add_annotation(text=str(row['BIKE_STANDS'])+\" Bike_Stands\",\n",
    "                       x=row['TRIPS'],\n",
    "                       y=row['NAME'],\n",
    "                       xanchor='left',\n",
    "                       showarrow=False)\n",
    "fig.update_layout(title_x=0.5)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf6f83-d7d1-48e8-a80c-93ccb39fd0bb",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "    The chart suggests that Dublin bikes is most popular in the city center. The busiest stations are all located in or near the city center, and there is a significant drop-off in usage in the suburbs.\n",
    "    Also, its possible to observe that some stations with fewer bike stands are actually in higher demand than their stand count suggests.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35411d54-b377-48a8-9a06-a32259a95288",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_by_station_capital = capital_df.groupby(['start_station_id'])['TRIPS'].sum().reset_index()\n",
    "trip_by_station_capital_join = pd.merge(trip_by_station_capital,  capital_stations,  how='inner',  right_on='station_id',  left_on='start_station_id').sort_values(by='TRIPS', ascending=False)\n",
    "\n",
    "top_10_capital = trip_by_station_capital_join.head(10)\n",
    "\n",
    "fig = px.bar(top_10_capital.sort_values(by='TRIPS', ascending = True), \n",
    "             x='TRIPS', \n",
    "             y='station_name',\n",
    "             labels=dict(x='TRIPS', y='station_name'),\n",
    "             title='Top 10 Bike Usage by Station - Washington, D.C.',\n",
    "             width=1000, \n",
    "             height=500, \n",
    "             orientation='h')\n",
    "\n",
    "fig.update_layout(title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1eb964-c03e-40b4-ad41-7bd66cbff386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "color_mapper = {\"OPEN\": \"#7CFC00\", \"CLOSED\": \"#DF0030\"}\n",
    "\n",
    "fig = px.scatter_mapbox(dublin_bike_stations, \n",
    "                        lat=\"LATITUDE\",\n",
    "                        lon=\"LONGITUDE\", \n",
    "                        color=\"STATUS\", \n",
    "                        size=\"BIKE_STANDS\", \n",
    "                        color_discrete_map=color_mapper,\n",
    "                        size_max=15, \n",
    "                        zoom=12, \n",
    "                        hover_data=[\"BIKE_STANDS\"],\n",
    "                        mapbox_style=\"open-street-map\")\n",
    "\n",
    "# Update layout for larger figure size\n",
    "fig.update_layout(\n",
    "    title=\"Bike Stations in Dublin\",\n",
    "    autosize=False,\n",
    "    width=1200, \n",
    "    height=600,  \n",
    ")\n",
    "fig.update_layout(title_x=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0a85e-395b-4690-bbc1-fd2b7a88d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.scatter_mapbox(capital_stations, \n",
    "                        lat=\"lat\",\n",
    "                        lon=\"long\", \n",
    "                        size_max=15, \n",
    "                        zoom=12, \n",
    "                        hover_data=[\"station_name\"],\n",
    "                        mapbox_style=\"open-street-map\")\n",
    "\n",
    "# Update layout for larger figure size\n",
    "fig.update_layout(\n",
    "    title=\"Bike Stations  - Washington, D.C.\",\n",
    "    autosize=False,\n",
    "    width=1200, \n",
    "    height=600,  \n",
    ")\n",
    "fig.update_layout(title_x=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b2bd0-d00d-4e76-ac71-9dd52b53eff7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Number of trips by weekday and Location\n",
    "The best approach to perform these chart, is using a dinamic apresentation, and for this purpose, a dashboart using dash and plotly was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b2dc2-e1db-48fd-bca9-10f8e34aee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input,Output\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the Plot for Dublin\n",
    "bike_usage['WEEKDAY'] = bike_usage['TIME'].dt.weekday\n",
    "dublin_weekady = bike_usage.groupby(['WEEKDAY']).agg(BIKE_STANDS=('BIKE_STANDS', 'sum'), \n",
    "                                         TRIPS=('TRIPS', 'sum')).reset_index().sort_values('WEEKDAY', ascending=True)\n",
    "# reneame the weedkday using a mpa \n",
    "dublin_weekady['WEEKDAY'] = dublin_weekady['WEEKDAY'].map({0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'})\n",
    "\n",
    "\n",
    "# Prepare Weekday dataset\n",
    "capital_df['WEEKDAY'] = capital_df['started_at'].dt.weekday\n",
    "\n",
    "\n",
    "capital_weekady = capital_df.groupby(['WEEKDAY']).agg(TRIPS=('TRIPS', 'sum')).reset_index().sort_values('WEEKDAY', ascending=True)\n",
    "\n",
    "\n",
    "# reneame the weedkday using a mpa \n",
    "capital_weekady['WEEKDAY'] = capital_weekady['WEEKDAY'].map({0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'})\n",
    "\n",
    "\n",
    "# Build App\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "Location = ['Dublin', \"Washington, D.C.\",]\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Trips by WeekDay and Location\"),\n",
    "    dcc.Graph(id='graph'),\n",
    "    \n",
    "    # create a dropdown\n",
    "    html.Label([ \"Location\",\n",
    "                \n",
    "            dcc.RadioItems(\n",
    "            id='day-dropdown',\n",
    "            # Select the unique days in dataframe\n",
    "            value='Dublin', options=[\n",
    "                {'label': loc, 'value': loc}\n",
    "                for loc in Location\n",
    "            ]\n",
    "        )\n",
    "    ]),\n",
    "])\n",
    "\n",
    "\n",
    "# Define callback to update graph\n",
    "@app.callback(\n",
    "     \n",
    "    Output('graph', 'figure'),\n",
    "    [Input(\"day-dropdown\", \"value\")]\n",
    "    \n",
    "   \n",
    ")\n",
    "\n",
    "# Function to return the chreated chart\n",
    "def update_figure(Location):\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    \n",
    "    # filter the dataframe    \n",
    "    if Location == \"Dublin\":\n",
    "        return px.bar(dublin_weekady, x='WEEKDAY', y='TRIPS', \n",
    "                     labels=dict(x='WEEKDAY', y='TRIPS'),\n",
    "                     title='Bike Usage by Weekday  - Dublin',\n",
    "                     width = 600, height = 500)\n",
    "    else:\n",
    "        \n",
    "        # Create Plots\n",
    "        return px.bar(capital_weekady, x='WEEKDAY', y='TRIPS', \n",
    "                             labels=dict(x='WEEKDAY', y='TRIPS'),\n",
    "                             title='Bike Usage by Weekday - Washington, D.C.',\n",
    "                             width = 600, height = 500)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7483674-b747-441e-85bb-2618174df8e9",
   "metadata": {},
   "source": [
    "# Trips by Month and Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381e46f-e812-41c1-b169-6839b836af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the Plot for Dublin\n",
    "bike_usage['MONTH'] = bike_usage['TIME'].dt.month\n",
    "dublin_month = bike_usage.groupby(['MONTH']).agg(TRIPS=('TRIPS', 'sum')).reset_index().sort_values('MONTH', ascending=True)\n",
    "# reneame the weedkday using a map\n",
    "dublin_month['MONTH'] = dublin_month['MONTH'].map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'})\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Weekday dataset\n",
    "capital_df['MONTH'] = capital_df['started_at'].dt.month\n",
    "\n",
    "\n",
    "capital_month = capital_df.groupby(['MONTH']).agg(TRIPS=('TRIPS', 'sum')).reset_index().sort_values('MONTH', ascending=True)\n",
    "\n",
    "\n",
    "# reneame the weedkday using a mpa \n",
    "capital_month['MONTH'] = capital_month['MONTH'].map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'})\n",
    "\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Trips by Month and Location\"),\n",
    "    dcc.Graph(id='month'),\n",
    "    \n",
    "    # create a dropdown\n",
    "    html.Label([ \"Location\",\n",
    "                \n",
    "            dcc.RadioItems(\n",
    "            id='day-dropdown',\n",
    "            # Select the unique days in dataframe\n",
    "            value='Dublin', options=[\n",
    "                {'label': loc, 'value': loc}\n",
    "                for loc in Location\n",
    "            ]\n",
    "        )\n",
    "    ]),\n",
    "])\n",
    "\n",
    "\n",
    "# Define callback to update graph\n",
    "@app.callback(\n",
    "     \n",
    "    Output('month', 'figure'),\n",
    "    [Input(\"day-dropdown\", \"value\")]\n",
    "    \n",
    "   \n",
    ")\n",
    "# Function to return the chreated chart\n",
    "def update_month(Location):\n",
    "        \n",
    "    # filter the dataframe    \n",
    "    if Location == \"Dublin\":\n",
    "        \n",
    "        return px.bar(dublin_month, x='MONTH', y='TRIPS', \n",
    "                     labels=dict(x='MONTH', y='TRIPS'),\n",
    "                     title='Bike Usage by Month  - Dublin',\n",
    "                     width = 600, height = 500)\n",
    "    else:\n",
    "        \n",
    "        # Create Plots\n",
    "        return px.bar(capital_month, x='MONTH', y='TRIPS', \n",
    "                             labels=dict(x='MONTH', y='TRIPS'),\n",
    "                             title='Bike Usage by Month - Washington, D.C.',\n",
    "                             width = 600, height = 500)\n",
    "        \n",
    "\n",
    "\n",
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f17634-e1d6-400c-abf7-0b4d50552936",
   "metadata": {},
   "source": [
    "### EDA For sentiment Analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f4197-a5c8-486d-bcac-522e4d24e22d",
   "metadata": {},
   "source": [
    "### Distribution of reviews lenths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b34e3-569d-41e8-8c6e-7fd110ab75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(1,2, figsize=(10, 4))\n",
    "\n",
    "# Calculating the length of each review\n",
    "dublin_reviews_df['txt_length'] = dublin_reviews_df['text'].apply(len)\n",
    "capital_reviews_df['txt_length'] = capital_reviews_df['text'].apply(len)\n",
    "\n",
    "# Plotting the distribution of Dublin review lengths\n",
    "sns.histplot(dublin_reviews_df['txt_length'], ax=axes[0], bins=25, kde=True,color=\"Blue\")\n",
    "axes[0].set_title(f'Dist. of reviews Lengths - Dublin - Tot:  {len(dublin_reviews_df)} reviews')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.histplot(capital_reviews_df['txt_length'], ax=axes[1],bins=25, kde=True,color=\"Blue\")\n",
    "axes[1].set_title(f'Dist. of reviews Lengths - Washington, D.C. Tot: {len(capital_reviews_df)} reviews')\n",
    "\n",
    "# Tighten the layout of the subplots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627ecfbe-496b-4848-a091-83827e4f17b2",
   "metadata": {},
   "source": [
    "    From this Comparison, its possible to observe that there are more loonger reviews for Dublin bikeshare service than for the Washington, D.C. bikeshare service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8bce5-4b7d-4694-80b9-2d420e4ac029",
   "metadata": {},
   "source": [
    "### Labeling the reviews using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9059d-da53-4ffe-b5f2-992fbd4ecdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not installed, its necessary to install textblob  \n",
    "# !pip install textblob   # To install uncoment this line\n",
    "from textblob import TextBlob\n",
    "\n",
    " \n",
    "# This function: textblob_sentiment_analysis takes a review as input and uses TextBlob to analyze its sentiment.\n",
    "\n",
    "def textblob_sentiment_analysis(review):\n",
    "    # Analyzing the sentiment of the review\n",
    "    sentiment = TextBlob(review).sentiment\n",
    "    # Classifying based on polarity\n",
    "    if sentiment.polarity > 0.1:\n",
    "        return 'Positive'\n",
    "    elif sentiment.polarity < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "    \n",
    "# Applying TextBlob sentiment analysis to the reviews\n",
    "capital_reviews_df['Sentiment'] = capital_reviews_df['text'].apply(textblob_sentiment_analysis)\n",
    "dublin_reviews_df['Sentiment'] = dublin_reviews_df['text'].apply(textblob_sentiment_analysis)\n",
    "\n",
    "   \n",
    "# Displaying the first few rows with the sentiment\n",
    "display(capital_reviews_df.head(3))\n",
    "display(dublin_reviews_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2424e-061d-4595-8cda-fc4b3c79dbd6",
   "metadata": {},
   "source": [
    "# Comparing the Sentment in both bike Systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e3d99-ce05-4b27-aca3-5429251073d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Analyzing the distribution of sentiments\n",
    "dublin_sentiment = dublin_reviews_df['Sentiment'].value_counts()\n",
    "capital_sentiment = capital_reviews_df['Sentiment'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Defining custom color palettes\n",
    "colors = {\n",
    "    'Negative': \"#FF0000\",  # Red for Negative sentiment\n",
    "    'Neutral': \"#0000FF\",   # Blue for Neutral sentiment\n",
    "    'Positive': \"#00FF00\"   # Green for Positive sentiment\n",
    "}\n",
    "\n",
    "\n",
    "# Plotting the distribution of sentiments for Dublin\n",
    "sns.barplot(x=dublin_sentiment.index, y=dublin_sentiment.values, ax=axes[0], palette=colors)\n",
    "axes[0].set_title('Distribution of Sentiments - Dublin')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Plotting the distribution of sentiments for Capital Bikeshare\n",
    "sns.barplot(x=capital_sentiment.index, y=capital_sentiment.values, ax=axes[1], palette=colors)\n",
    "axes[1].set_title('Distribution of Sentiments - Capital Bikeshare')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460eed92-d0f5-4203-8954-0518790e039f",
   "metadata": {},
   "source": [
    "# Insight:\n",
    "\n",
    "    The graphs show that the sentiment for both: Dublin and Washington D.C is  positive. However, there is a small percentage of negative sentiment for both services.\n",
    "    Also, for Capital Bikeshare, there are more neutral sentiment than positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c977b1-f695-4eab-ad10-0ad26b06db1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.0 Statistics for Data Analytics\n",
    "### Descriptive statistics are a set of tools and techniques used to summarize and describe the key features of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e6c14-bc53-424a-aa93-4279731f3c0e",
   "metadata": {},
   "source": [
    "### Distribution and Density of Dubin bike_usage Dataframe\n",
    "    plotting the Distribution of the dataset: “bike_usage”, it represents the bike usage in Dublin Bike System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfdd73-0b67-41e7-82bb-f102a89ba804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Plot the distributions on the subplots\n",
    "\n",
    "std_dev = bike_usage[\"BIKE_STANDS\"].std()\n",
    "mean = bike_usage['BIKE_STANDS'].mean()\n",
    "sns.distplot(bike_usage['BIKE_STANDS'], ax=axes[0, 0], label='Bike Stands')\n",
    "axes[0, 0].set_title('Distribution of Bike Stands')\n",
    "axes[0, 0].axvline(bike_usage['BIKE_STANDS'].mean(), color='red', linestyle='--', label=(f'Mean:{mean}'))\n",
    "axes[0, 0].axvline(bike_usage['BIKE_STANDS'].std(), color='orange', linestyle='--', label= (f'Std: {std_dev}'))\n",
    "axes[0, 0].legend()\n",
    "\n",
    "\n",
    "\n",
    "std_dev = bike_usage[\"BIKES_IN_USE\"].std()\n",
    "mean = bike_usage['BIKES_IN_USE'].mean()                 \n",
    "sns.distplot(bike_usage['BIKES_IN_USE'], kde=True, ax=axes[0, 1], label='Bikes in Use')\n",
    "axes[0, 1].set_title('Distribution of Bikes in Use')\n",
    "axes[0, 1].axvline(bike_usage['BIKES_IN_USE'].mean(), color='red', linestyle='--', label=(f'Mean:{mean}'))\n",
    "axes[0, 1].axvline(bike_usage['BIKES_IN_USE'].std(), color='orange', linestyle='--', label=(f'Std: {std_dev}'))\n",
    "axes[0, 1].legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "std_dev = bike_usage[\"AVAILABLE_BIKES\"].std()\n",
    "mean = bike_usage['AVAILABLE_BIKES'].mean()\n",
    "sns.distplot(bike_usage['AVAILABLE_BIKES'], kde=True, ax=axes[1, 0], label='Available Bikes')\n",
    "axes[1, 0].set_title('Distribution of Available Bikes')\n",
    "axes[1, 0].axvline(bike_usage['AVAILABLE_BIKES'].mean(), color='red', linestyle='--', label=(f'Mean:{mean}'))\n",
    "axes[1, 0].axvline(bike_usage['AVAILABLE_BIKES'].std(), color='orange', linestyle='--', label=(f'Std: {std_dev}'))\n",
    "axes[1, 0].legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "std_dev = bike_usage[\"TRIPS\"].std()\n",
    "mean = bike_usage['TRIPS'].mean()\n",
    "sns.distplot(bike_usage['TRIPS'], kde=True, ax=axes[1, 1], label='Trips')\n",
    "axes[1, 1].set_title('Distribution of Trips')\n",
    "axes[1, 1].axvline(bike_usage['TRIPS'].mean(), color='red', linestyle='--', label=(f'Mean:{mean}'))\n",
    "axes[1, 1].axvline(bike_usage['TRIPS'].std(), color='orange', linestyle='--', label=(f'Std: {std_dev}'))\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Tighten the layout of the subplots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa194ae-0dd9-4426-bba1-c15170ddb231",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Insights\n",
    "    Distribution of Bike Stands by Density\n",
    "\n",
    "    The chart shows that the majority of bike stands have a density of between 10 and 25 per unit area. \n",
    "\n",
    "    However, there is a small number of stations with a significantly higher or lower number of bike stands than the average. \n",
    "    This suggests that there may be a need to redistribute bike stands to ensure that all stations have a similar number of bike stands available.\n",
    "\n",
    "\n",
    "    Distribution of Bikes in Use\n",
    "\n",
    "    From the Distribution of Bikes in Use, with a mean of 20.65 and a standard deviation of 10.33, its possible assume that the majority of bikes in use have a usage of between 10.33 and 30.97. There is a smaller number of bikes that are used more or less than this range.\n",
    "    This suggests that bikes are typically used for a variety of purposes, from short commutes to longer recreational rides..\n",
    "\n",
    "    Distribution of Available Bikes\n",
    "\n",
    "    From this chart, its observed that the majority of available bikes have a density of between 10 and 20. Also, there are a smaller number of available bikes with densities below 10 or above 20.\n",
    "    That means, the available bikes are typically concentrated in certain areas, rather than being evenly distributed throughout the area.\n",
    "\n",
    "\n",
    "    Distribution of Trips\n",
    "\n",
    "    The distribution of trips is skewed to the right, with a mean of 2.2 and a standard deviation of 2.35. This indicates that there are more times when there are fewer trips than times when there are more trips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702d420-1170-4959-98f1-1365f37c0983",
   "metadata": {},
   "source": [
    "# Distribution and Density of Trips grouped by station on Both Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda42e5-7be6-4a86-9608-5d218fe77547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# grpouping Cpitaol_DF trips by station_id\n",
    "capital_grp_trips = capital_df.groupby('start_station_id')['TRIPS'].sum().reset_index()\n",
    "\n",
    "# grpouping Dublin bike_usage df  by station_id\n",
    "dublin_grp_trips = bike_usage.groupby('STATION ID')['TRIPS'].sum().reset_index()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "# Plot the distributions on the subplots\n",
    "std_dev = capital_grp_trips[\"TRIPS\"].std()\n",
    "mean = capital_grp_trips['TRIPS'].mean()               \n",
    "sns.distplot(capital_grp_trips['TRIPS'], ax=axes[0], label='Bike Stands')\n",
    "axes[0].set_title('Distribution of Trips by Station ID  - Capital DF')\n",
    "axes[0].axvline(capital_grp_trips['TRIPS'].mean(), color='red', linestyle='--', label=(f'Mean:{mean}'))\n",
    "axes[0].axvline(capital_grp_trips['TRIPS'].std(), color='orange', linestyle='--', label= (f'Std: {std_dev}'))\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "std_dev = dublin_grp_trips[\"TRIPS\"].std()\n",
    "mean = dublin_grp_trips['TRIPS'].mean()\n",
    "sns.distplot(dublin_grp_trips['TRIPS'], kde=True, ax=axes[1], label='Trips')\n",
    "axes[1].set_title('Distribution of Trips by Station ID - Dublin Bikes')\n",
    "axes[1].axvline(dublin_grp_trips['TRIPS'].mean(), color='red', linestyle='--', label=(f'Mean:{mean}'))\n",
    "axes[1].axvline(dublin_grp_trips['TRIPS'].std(), color='orange', linestyle='--', label=(f'Std: {std_dev}'))\n",
    "axes[1].legend()\n",
    "\n",
    "\n",
    "# Tighten the layout of the subplots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b82e5-5bfd-491f-9523-8b84730060dc",
   "metadata": {},
   "source": [
    "    Its possible to see in both results that the distribution is positively skewed, showing that there are a few stations with a very high number of trips and others stations with a lower number of trips.\n",
    "    Also, the mean number of trips per station is higher for Capital DF (5240) than for Dublin Bikes (9197), this indicates that the average station in Capital DF has more trips than Dublin Station.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf1817-aa66-4aec-b3d5-3e053475890c",
   "metadata": {},
   "source": [
    "# Correlation between features in Dublin bike_share dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52bea9-5a28-4781-b6e0-b157ee6423ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Correlation between Bike Stands and Bikes in Use\n",
    "corr = bike_usage[['BIKE_STANDS', 'BIKES_IN_USE', 'AVAILABLE_BIKES', 'TRIPS', 'MONTH', 'WEEKDAY']].corr()\n",
    "\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ceed2-1ae0-4108-80a0-ce1780d32fdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# From this correlation heatmap, its possigle to observe these insights. \n",
    "\n",
    "    There is a strong negative correlation between BIKE_STANDS and BIKES_IN_USE. This means that as the number of bike stands at a station increases, the number of bikes in use at that station decreases. \n",
    "    There is a moderate negative correlation between AVAILABLE_BIKES and TRIPS. This means that as the number of available bikes at a station decreases, the number of bike trips starting  from station increases. \n",
    "    there is a very weak correlation between MONTH and TRIPS. This suggests that the number of bike trips does not vary significantly throughout the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1412d5-015d-44c9-ab9f-16b1aff6707e",
   "metadata": {},
   "source": [
    "# Frequency distribution of WeekDay - Comparing Dublin Dataset to Capital_Df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1454c-6111-4052-b34f-88d8d6e594bf",
   "metadata": {},
   "source": [
    "### Dublin bike_usage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29826a65-2f99-4c04-be84-bff4bafb1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt1\n",
    "plt.figure(figsize=(8, 6))\n",
    "bike_usage['WEEKDAY'].map({0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}).value_counts().plot(kind='bar')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency distribution of use by Weekday - Dublin Bike Share')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8880121-2717-4428-80d9-c0a1de24a14b",
   "metadata": {},
   "source": [
    "### Capital_df Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe33676-b449-4363-b6e5-8d5fae184852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "capital_df['WEEKDAY'].map({0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}).value_counts().plot(kind='bar')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency distribution of use by Weekday - Capital_df')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed989d54-80f4-491c-bea2-aa0b56bdd64e",
   "metadata": {},
   "source": [
    "  # Insights from these plots\n",
    "    The bike usage in Dublin peaks on Thursdays, with a dip on Sundays, whereas n Capital DF, Thursday is also the most popular weekday, but Monday is the least popular, highlighting that there is a different behavior in each city."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409cf766-6887-4967-aabd-d2c79d38f72d",
   "metadata": {},
   "source": [
    "# Inferential Statistics \n",
    "    Getting the confidence interval for the population proportion of trips per month.\n",
    "    \n",
    "    The population proportion of trips per month is a categorical variable, so we can use the chi-squared test to gain insights into the possible population values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39311171-5011-49b0-b52e-c254643acf8f",
   "metadata": {},
   "source": [
    "# Calculating tht confidence interval for the population proportion of trips per month on both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23ca55-4dd0-48fb-ab02-5672e390a43c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dublin bike_usage Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d7af0-2190-4fc6-97dd-b92d697e14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "\n",
    "# Calculate Proportion of Bike Trips for Each Month - Dublin Bike share Dataset\n",
    "# --------------------------------------------------------------\n",
    "monthly_trips =  bike_usage[bike_usage['MONTH']!=12].groupby('MONTH')['TRIPS'].sum()\n",
    "total_trips =  bike_usage['TRIPS'].sum()\n",
    "proportions = monthly_trips / total_trips\n",
    "\n",
    "# Calculate Confidence Intervals for Proportions\n",
    "conf_intervals = proportion_confint(count=monthly_trips, nobs=total_trips, alpha=0.05, method='normal')\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame()\n",
    "results['TRIPS'] = monthly_trips\n",
    "results['Proportion'] = proportions.values\n",
    "results['Lower CI'] =  conf_intervals[0]\n",
    "results['Upper CI'] =  conf_intervals[1]\n",
    "results = results.reset_index()\n",
    "\n",
    "  \n",
    "# Plot the proportion vs month and overlay the confidence intervals\n",
    "\n",
    "ax0.plot(results['MONTH'], results['Proportion'], marker='o', linestyle='-')\n",
    "ax0.fill_between(results['MONTH'], results['Lower CI'], results['Upper CI'], alpha=0.2)\n",
    "\n",
    "# Set labels and title\n",
    "ax0.set_xlabel('MONTH')\n",
    "ax0.set_ylabel('Proportion of Bike Trips')\n",
    "ax0.set_title('Proportions and Confidence Intervals Dublin bike_usage dataframe')\n",
    "\n",
    "# -------------------------------------------- End Dublin Bike Share Dataset\n",
    "\n",
    "# Calculate Proportion of Bike Trips for Each Month - Capital DF Dataset\n",
    "monthly_trips =  capital_df.groupby('MONTH')['TRIPS'].sum()\n",
    "total_trips =  capital_df['TRIPS'].sum()\n",
    "proportions = monthly_trips / total_trips\n",
    "\n",
    "# Calculate Confidence Intervals for Proportions\n",
    "conf_intervals = proportion_confint(count=monthly_trips, nobs=total_trips, alpha=0.05, method='normal')\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame()\n",
    "results['TRIPS'] = monthly_trips\n",
    "results['Proportion'] = proportions.values\n",
    "results['Lower CI'] =  conf_intervals[0]\n",
    "results['Upper CI'] =  conf_intervals[1]\n",
    "results = results.reset_index()\n",
    "\n",
    "  \n",
    "# Plot the proportion vs month and overlay the confidence intervals\n",
    "\n",
    "ax1.plot(results['MONTH'], results['Proportion'], marker='o', linestyle='-')\n",
    "ax1.fill_between(results['MONTH'], results['Lower CI'], results['Upper CI'], alpha=0.2)\n",
    "\n",
    "# Set labels and title\n",
    "ax1.set_xlabel('MONTH')\n",
    "ax1.set_ylabel('Proportion of Bike Trips')\n",
    "ax1.set_title('Monthly Bike Trip Proportions and Confidence Intervals Capital_df dataframe')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2acc31-710b-42cc-8705-4d572dbc5d20",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "\n",
    "    The proportion of bike trips in Dublin varies throughout the year, with the highest proportion of trips occurring in June, July, August.The proportion of bike trips in Dublin has been increasing over time, while the proportion of bike trips in Capidal_DF has remained stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30040c1-e86f-4516-a72f-810faef986a8",
   "metadata": {},
   "source": [
    "### Finding similarities between Dublin (bike_usage) and Washington, D.C. (Capital_Df).\n",
    "    Applying apply parametric and non-parametric inferential statistical techniques(t-test, analysis of variance, Wilcoxon test, chi-squared test, among others).\n",
    "    \n",
    "    #For this purpose, its necessary become both datasets similar eacah other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772c33e-d74f-41fd-8d0d-92d530e1817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_df = capital_df[[ 'start_station_id','started_at',  'WEEKDAY', 'MONTH', 'TRIPS'  ]]\n",
    "bike_usage = bike_usage[[ 'STATION ID', 'TIME', 'WEEKDAY', 'MONTH', 'TRIPS']  ]\n",
    "display(capital_df.head(2))\n",
    "display(bike_usage.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cec675-e813-4ef4-bb89-9559cd886f10",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "    Sampling data is a fundamental practice in statistics and data analysis. When dealing with large populations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c00c8-7887-4f2e-8254-ce7ba56b09bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, wilcoxon, chi2_contingency, f_oneway, kruskal\n",
    "\n",
    "# Randomly sample 20% of bike_usage  dataset\n",
    "bike_usage_sample = bike_usage.sample(frac=0.05, random_state=42) \n",
    "bike_usage_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Randomly sample 20% of capital_df dataset\n",
    "capital_df_sample = capital_df.sample(frac=0.05, random_state=42) \n",
    "capital_df_sample.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d01ff9-26bc-413f-b46e-8542aced0860",
   "metadata": {},
   "source": [
    "# Evaluation of the sample size\n",
    "\n",
    "    Fof validating the sample size, a ECDFs (Empirical Cumulative Distribution Function) was applied in both dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03af5a-7d16-47d1-85d6-a79c1c4a62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(data)+1) / len(data)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# ecdf for Dublin bike_usage\n",
    "d_x_pop, d_y_pop = ecdf(bike_usage['TRIPS'])\n",
    "d_x_sample, d_y_sample = ecdf(bike_usage_sample['TRIPS'])\n",
    "\n",
    "\n",
    "# ecdf for Capital DF\n",
    "c_x_pop, c_y_pop = ecdf(capital_df['TRIPS'])\n",
    "c_x_sample, c_y_sample = ecdf(capital_df_sample['TRIPS'])\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot Empirical Cumulative Distribution Function) - bike_usage\n",
    "ax0.plot(d_x_pop, d_y_pop, marker='.', linestyle='none', color='blue', label='Bike_usage Population')\n",
    "ax0.plot(d_x_sample, d_y_sample, marker='.', linestyle='none', color='red', label='Bike_usage Sample - 5%')\n",
    "ax0.set_title('ECDFs (Empirical Cumulative Distribution Function) - bike_usage' )\n",
    "ax0.legend()\n",
    "\n",
    "\n",
    "# Plot Empirical Cumulative Distribution Function) - Capital_df\n",
    "ax1.plot(c_x_pop, c_y_pop, marker='.', linestyle='none', color='green', label='Capital_df Population')\n",
    "ax1.plot(c_x_sample, c_y_sample, marker='.', linestyle='none', color='yellow', label='Capital_df Sample')\n",
    "ax1.set_title('ECDFs (Empirical Cumulative Distribution Function) - Capital_df' )\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2138d-55dc-4a58-b977-2fe2adae4c6b",
   "metadata": {},
   "source": [
    "    This sample size of dublin bike_usage doesnt represents the population, it should be increased once the population is much larger than the sample.  so It was applied 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1973dac-d9e1-470d-b04c-4a28ecbd8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, wilcoxon, chi2_contingency, f_oneway, kruskal\n",
    "\n",
    "# Randomly sample 20% of bike_usage  dataset\n",
    "bike_usage_sample = bike_usage.sample(frac=0.2, random_state=42) \n",
    "bike_usage_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Randomly sample 20% of capital_df dataset\n",
    "capital_df_sample = capital_df.sample(frac=0.2, random_state=42) \n",
    "capital_df_sample.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855ca07-24a3-492f-9231-bd47ef744c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ecdf for Dublin bike_usage\n",
    "d_x_pop, d_y_pop = ecdf(bike_usage['TRIPS'])\n",
    "d_x_sample, d_y_sample = ecdf(bike_usage_sample['TRIPS'])\n",
    "\n",
    "\n",
    "# ecdf for Capital DF\n",
    "c_x_pop, c_y_pop = ecdf(capital_df['TRIPS'])\n",
    "c_x_sample, c_y_sample = ecdf(capital_df_sample['TRIPS'])\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot Empirical Cumulative Distribution Function) - bike_usage\n",
    "ax0.plot(d_x_pop, d_y_pop, marker='.', linestyle='none', color='blue', label='Bike_usage Population')\n",
    "ax0.plot(d_x_sample, d_y_sample, marker='.', linestyle='none', color='red', label='Bike_usage Sample - 20%')\n",
    "ax0.set_title('ECDFs (Empirical Cumulative Distribution Function) - bike_usage' )\n",
    "ax0.legend()\n",
    "\n",
    "\n",
    "# Plot Empirical Cumulative Distribution Function) - Capital_df\n",
    "ax1.plot(c_x_pop, c_y_pop, marker='.', linestyle='none', color='green', label='Capital_df Population')\n",
    "ax1.plot(c_x_sample, c_y_sample, marker='.', linestyle='none', color='yellow', label='Capital_df Sample')\n",
    "ax1.set_title('ECDFs (Empirical Cumulative Distribution Function) - Capital_df' )\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06974862-067e-4a77-a523-c3b9ff6ca865",
   "metadata": {},
   "source": [
    "# With 20% the sample now is in a good size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4fd89-6984-4ebe-886b-8be5745e5ad8",
   "metadata": {},
   "source": [
    "# Parametric Tests:\n",
    "\n",
    "    Since the data is about trips at different times and stations, A t-test was applied to compare the means of the trips between Dublin and Washington, D.C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba395e-5e98-4033-8024-cd3f1e882841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency\n",
    "\n",
    "# Parametric Test (t-test)\n",
    "t_stat, p_val = ttest_ind(bike_usage_sample['TRIPS'], capital_df_sample['TRIPS'])\n",
    "print(f\"T-test: t-statistic = {t_stat}, p-value = {p_val}\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\nConclusions:\")\n",
    "if p_val < 0.05:\n",
    "    print(\"There is a significant difference between the two datasets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the two datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568fc45-2aea-4b2a-8c7a-d01defe7b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Parametric Test (Wilcoxon)\n",
    "u_stat, p_val = mannwhitneyu(bike_usage_sample['TRIPS'], capital_df_sample['TRIPS'])\n",
    "print(f\"Wilcoxon test: U-statistic = {u_stat}, p-value = {p_val}\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\nConclusions:\")\n",
    "if p_val < 0.05:\n",
    "    print(\"There is a significant difference between the two datasets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the two datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553a5c0-77a3-4406-8179-4aa63bb4059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Squared Test\n",
    "# Assuming we want to compare the weekday distribution between the two datasets\n",
    "contingency_table = pd.crosstab(bike_usage_sample['WEEKDAY'], bike_usage_sample['WEEKDAY'])\n",
    "chi2, p_val, _, _ = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-Squared Test: Chi2 statistic = {chi2}, p-value = {p_val}\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\nConclusions:\")\n",
    "if p_val < 0.05:\n",
    "    print(\"There is a significant difference between the two datasets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the two datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edff3e-f686-4226-928e-ea327ab24a1d",
   "metadata": {},
   "source": [
    "### Hypothesis: Comparing Bike Usage in Dublin and Washington D.C. During Daytime\n",
    "    The hypothesis in question posits that Dublin experiences more bike rides during daytime hours compared to Washington D.C.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0bc73-8a1b-412c-9122-c807ae08b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test this hypotesis, its necessary to filter the dataframe by hour.\n",
    "# Subset data for daytime (6 am to 6 pm)\n",
    "bike_usage_sample['HOUR'] = bike_usage_sample['TIME'].dt.hour\n",
    "dublin_group  = bike_usage_sample.groupby(['HOUR'])['TRIPS'].sum().reset_index()\n",
    "capital_df_sample['HOUR'] = capital_df_sample['started_at'].dt.hour\n",
    "capital_group = capital_df_sample.groupby(['HOUR'])['TRIPS'].sum().reset_index()\n",
    "\n",
    "\n",
    "dublin_sample_daytime = dublin_group[(dublin_group['HOUR'] >= 6) & (dublin_group['HOUR'] < 18)]\n",
    "capital_sample_daytime= capital_group[(capital_group['HOUR'] >= 6) & (capital_group['HOUR'] < 18)]\n",
    "\n",
    "\n",
    "# Calculate total trips for each city during daytime\n",
    "total_trips_dublin = dublin_sample_daytime['TRIPS'].sum()\n",
    "total_trips_capital = capital_sample_daytime['TRIPS'].sum()\n",
    "\n",
    "print(f\"Total trips in Dublin during daytime: {total_trips_dublin}\")\n",
    "print(f\"Total trips in Washington D.C. during daytime: {total_trips_capital}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43867e06-9df7-4365-9458-2e9251229fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing (comparing means using t-test)\n",
    "from scipy import stats\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(dublin_sample_daytime['TRIPS'], capital_sample_daytime['TRIPS'], equal_var=False)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: Dublin has more bike usage during daytime than Washington D.C.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference in bike usage during daytime between Dublin and Washington D.C.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1b9a5-ef05-446e-975d-a19003f48c4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5dbfc7-406f-4fbd-b326-7bed5f00f5a0",
   "metadata": {},
   "source": [
    "## Utilizing Time Series to Test and Predict bike usage in Dublin bikeshare dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520211e-53ec-496f-a7ef-5003547525c7",
   "metadata": {},
   "source": [
    "    Filter the dataset to apply in a month (november)\n",
    "    Also, its necessary to create a series object with time(datetime) as index, and sum of trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e8ca5-e1f3-461a-b908-d7a032d28629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "bike_usage_filter = bike_usage[bike_usage['TIME'].dt.month == 11]\n",
    "bike_series = bike_usage_filter.groupby(['TIME'])['TRIPS'].sum()\n",
    "\n",
    "# Set the size of the data set\n",
    "plt.figure(figsize = (15, 3))\n",
    "\n",
    "# Arrange the data set using date_range() method\n",
    "xticks = pd.date_range(start = bike_series.index.min(), end = bike_series.index.max(), freq = 'D')\n",
    "\n",
    "# Set the date format for the plot\n",
    "#plt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation = 60, ha = \"left\")\n",
    "plt.xticks(xticks, xticks.strftime(\"%a-%d-%m\"), rotation = 60, ha = \"left\")\n",
    "\n",
    "plt.plot(bike_series, linewidth = 1)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41403fe-432d-4367-ae48-f8bd0e13cda6",
   "metadata": {},
   "source": [
    "### Function to evaluate and plot the prediction, it use the features, target and regressor, also it uses (70%) of data points for training and the rest for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31dda0-2d28-4702-80b4-e9356abf29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# function to evaluate and plot the prediction, it use the features, target and regressor, also it uses (70%) of data points for training and the rest for testing\n",
    "\n",
    "def eval_on_features(features, target, regressor):\n",
    "   \n",
    "    n_train = round(len(features) * 0.7)\n",
    "    \n",
    "    # split the given features into a training and a test set\n",
    "    X_train, X_test = features[:n_train], features[n_train:]\n",
    "    \n",
    "    # also split the target array\n",
    "    y_train, y_test = target[:n_train], target[n_train:]    \n",
    "    regressor.fit(X_train, y_train)\n",
    "       \n",
    "    y_pred = regressor.predict(X_test)\n",
    "    y_pred_train = regressor.predict(X_train)\n",
    "    \n",
    "    \n",
    "    # Plotting the chart\n",
    "    plt.figure(figsize = (15, 3))\n",
    "    plt.xticks(range(0, len(X), 40), xticks.strftime(\"%a %m-%d\"), rotation = 60, ha = \"left\")\n",
    "    plt.plot(range(n_train), y_train, label = \"train\")\n",
    "    plt.plot(range(n_train, len(y_test) + n_train), y_test, '-', label = \"test\")\n",
    "    plt.plot(range(n_train), y_pred_train, '--', label = \"prediction train\")\n",
    "    plt.plot(range(n_train, len(y_test) + n_train), y_pred, '--', label = \"prediction test\")\n",
    "    plt.legend(loc = (1.01, 0))\n",
    "    plt.title(regressor)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Trips\")\n",
    "    # End of the chart\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Printting the R^2 and MSE result\n",
    "    print(f\"Model: {regressor}\")\n",
    "\n",
    "    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\n",
    "        \n",
    "    mse_test = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Test-set MSE: {:.2f}\".format(mse_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27913f8c-364f-4312-95ed-cfb28df77ffb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Applying RandomForesRegressor over the entire timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d51cf-ce7d-4e0a-8797-4bb0b68fdc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Extracting the entire timestamp in nanoseconds (POSIX time )\n",
    "# extract the target values (number of trips)\n",
    "y = bike_series.values\n",
    "\n",
    "# convert to POSIX time by dividing by 10**9\n",
    "X = bike_series.index.astype(\"int64\").values.reshape(-1, 1) // 10**9\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create an object 'regressor' \n",
    "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "\n",
    "# Call the function eval_on_features() to predict and calculate the R^2  and the mean_squared_error\n",
    "eval_on_features(X, y, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea79ffc-4c05-4da2-a934-6ce8ecc305e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "    After Applying this model over the entire timestamp, the result was:\n",
    "    R-squared value of -0.15 shows that its is not a good model, the model performed poorly and it did not explain the variance in the target variable.\n",
    "    A MSE of 2395.29 ondicates that,the difference between the predicted number of trips and the actual number of trips is high, it shows a larger discrepancy between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8f856-2913-4aba-8841-6574ed5ea3e8",
   "metadata": {},
   "source": [
    "# Aplying RandomForesRegressor over the Hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e397e0f-446a-480f-b09d-72cb9c3c4ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the data hourly in X_hour and make one column using reshape(-1, 1)\n",
    "X_hour = bike_series.index.hour.values.reshape(-1, 1)\n",
    "\n",
    "# Call the method eval_on_features() to calculate the R^2  and the mean_squared_erroreval_on_features(X_hour, y, regressor)\n",
    "eval_on_features(X_hour, y, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449f609-c40e-48dd-ae88-7cd9bb255aad",
   "metadata": {},
   "source": [
    "    By using the hour of the day as a feature, the RandomForestRegressor model achieved a moderate fit (R^2 of 0.52) and an average prediction error (MSE of 1010.20). which yet is a poor result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad56f4-3b42-469d-9936-e21b65a0d9cc",
   "metadata": {},
   "source": [
    "## Applying RandomForesRegressor over the day of the week and the hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac5e3a-1064-46a7-ae18-368e1c234685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Store the hourly data weekly in X_hour_week and make one column using reshape(-1, 1)\n",
    "X_day_hour_week = np.hstack([bike_series.index.dayofweek.values.reshape(-1, 1), bike_series.index.hour.values.reshape(-1, 1)])\n",
    "\n",
    "# Call the method eval_on_features() to calculate the R^2  and the mean_squared_erroreval_on_features(X_hour, y, regressor)\n",
    "eval_on_features(X_day_hour_week, y, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f35434-46cb-4284-8632-5526342a4720",
   "metadata": {},
   "source": [
    "    By applying both features, day of the week and hour, model achieved better result, a strong fit (0.80) and a reduced the MSE (408.83).\n",
    "    What means that taking both factores, the model can provide a better prediction of bike trips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361478bb-ef0a-488c-abd4-54718ad39d61",
   "metadata": {},
   "source": [
    "# Applying Linear Regression over the day of the week and the hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9bdf23-589c-474c-aa14-8a157671cce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import LinearRegression library from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Call the method eval_on_features() to calculate the R^2  and the mean_squared_erroreval_on_features(X_hour, y, regressor)\n",
    "eval_on_features(X_day_hour_week, y, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f3db1-c13d-4bdd-86f1-6a04cdfdd8b6",
   "metadata": {},
   "source": [
    "    By applying linear regression, the model achieved a bad result, R^2 value of 0.08 and a high prediction error MSE of 2071.39."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45b84f-c488-43d1-87d3-a8080ac0868e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Converting categorical data into numerical representation, applying One-hot encoding.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4b00e-0c53-4710-ac1c-4b32b10fe376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OneHotEncoder from sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Declare an object (enc) by calling a method OneHotEncoder()\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "# Create an array by calling a fit() method\n",
    "X_day_hour_week_onehot = enc.fit_transform(X_day_hour_week).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f19006-36fa-48e4-a17d-cdd99f7f5a95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using a combination of polynomial features and ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485421eb-bcf8-47c4-aec1-3c846a17efa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polynomial library from sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create and initialise an object named as PolynomialFeatures()\n",
    "poly_transformer = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\n",
    "\n",
    "# Call fit() method to train the model\n",
    "X_day_hour_week_onehot_poly = poly_transformer.fit_transform(X_day_hour_week_onehot)\n",
    "\n",
    "# Initialise an object by calling a method Ridge()\n",
    "lr = Ridge()\n",
    "\n",
    "# Call a method eval_on_features() to evaluate R^2 and plot \n",
    "eval_on_features(X_day_hour_week_onehot_poly, y, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641084e3-ea6c-4698-9956-beec6f41f4e7",
   "metadata": {},
   "source": [
    "    By transforming the categorical data into numerical using one-hot encoding and incorporating polynomial features in combination with Ridge regression, the model have improved. With R^2 of 0.79 and  MSE value of 463.64. this model performed similar RandomForesRegressor over the day of the week and the hour of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52435b08-17bf-4e0f-89c9-883720018b86",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "    From these 3 models, its possible to observe that RandomForestRegressor model performed better than other two. The Ridge regression, with its regularization and polynomial features performed similar, but slightly behind the RandomForestRegressor. whereas the LinearRegression model performed worse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fa0ed-2d70-4432-a079-67011226aa6b",
   "metadata": {},
   "source": [
    "# Sentimental Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
